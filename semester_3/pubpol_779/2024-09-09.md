## 2024-09-09

### Interpretation with Odds Ratios

The logit model can be written sucht that:
$$
ln \Omega = x_i \beta
$$
Where $\omega(x)$ is the odds of an event given x:
$$
\Omega(x) = \frac{P(y=1|x)}{P(y=0|x)}
$$

$ln \Omega$ is thus the log of the odds ratio. 

### Interpretation with Odds Ratios, 2

Since the model is linear in $\beta_k$:
$$
\frac{\delta ln \Omega(x)}{\delta x_k} = \beta_k
$$

- A one-unit change in $x_k$ is associated with a $\beta_k$ changte in the log odds ratio.
- Thus a one-unit change in $x_k$ is associated with an $exp(\beta_k)$ change in the odds ratio.
- If $exp(\beta_k) > 1$, the odds are $exp(\beta_k)$ times larger. If the odds ratio is less than 1, the odds are $exp(\beta_k)$ times smaller.

tldr: positive effect is larger than 1, negative effect is smaller than 1, because it is centered around 1 once you exponentiate the betas.

### Logistic Regression

- We can get the coefficients reported as the changes in the odds ratio using the `logistic` command (NOT `logit`)
- in other words, `logit` gives us $\beta_k$, `logistic` gives us a coefficient that equals $exp(\beta_k)$

#### Output Interpretation

- When a coefficient is less than one, it would be a negative effect.
    - Take the original odds for a given scenario, multiply by the coefficient, and you get the new odds.
- When a coefficient is greater than one, it would be a positive effect.

### Example

- Initial offs of a person voting for Biden are 7 to 3. That is an odds ratio of 2.333 and a probability of 7/7+3 or .70.
- If the Ideology score increases by 1, the odds ratio would change by a factor of .4073, making it (2.333 * .4073) = .9052.
- That's odds of .9052 to 1, which is about .48 in probability terms:
$$
\frac{.9052}{1+.9052} \approx .48
$$

- predicted probabilities are probably easier to interpret, but some disciplines (public health) tend to use odds ratios.

### Advice
- within the same model, offs ratios have some value in communicating relative risks for different groups.
- even in fields like health research, however the use of odds ratios is under question. (e.g. article by norton et al.)
- oddds ratios are not comparable across different models or studies.
- predicted probabilities and marginal effects are much more understandable to most audiences.

### Concluding points
- with dichotomous dependent variables, both the LPM and probit/logit models have virtues
- probit/logit models are better for unbiased predictions of marginal effects for specific hypothetical cases
- the LPM offers easier interpretation of coefficients
- When RCTs have a dichotomous DV, the LPM is just fine.

### Ordered Dependent Variables

We now look at cases where the dependent variable is ordinal: a ranked set of categories:
- e.g. a list of political parties across an ideological spectrum
- answers to a survey question such as: strongly agree, agree, neutral, disagree, strongly disagree

These variables are not continuous, nor do they have a normal distribution. 

### Example Lin and Fan (2020)

- bike ridership in charlotte, NC
- based on crowdsourced data from apps
- dependent variable (bicycle volume on the road): low, low/average, average, average/high, high
- independent variables: road type, slope, neighborhood demographics, bicycle facilities, time of day, etc

### General guide to interpretation

- the intercepts are estimated cut points between the categories
- the signs on the coefficients indicates the direction of the effect of an independent variable
- positive coefficients mean the probability mass moves up the ordinal scale as the independent variable increases.
- in this case, that means increasible probability a road will be in a higher category of road usage.
- the opposite is true for negative coefficients.

### Key Findings
- Bike traffic is lower on weekdays and higher during the 6amâ€“6pm time period.
- Longer road segments (+); roads with more through lanes (+); one-way streets (+); road slope (-)
- Neighborhood household income (+); total # households (+); total # families (-)
- suggested bike lanes (+); suggested bike routes with lesser safety features (-); greenways (+)

### Critique
- choice of ordered probit is a bit dubious
- data coming from cyclists provide volume of travel on each road segment during each hour
- this is interval level data they convert into ordinal categories:
    - low (0-39), low/average (40-79), average (80-119), average/high (120-159), high (160-200)
- why not just use OLS with the original data?
- in this case, categories create arbitrary divisions in the data.
- to work, ordered probit assumes a latent $y^*$ that maps onto the categories. **We already have an observed variable that does this**

### We again use a latent variable $Y^*$

- $y^* = x_i \beta + \epsilon_i$ ranges from $-\infty$ to $\infty$ and maps onto observed variable $y$
- if $y$ has h categories, then there are J-1 cut points along $Y^*$ that map values of $y*$ onto $y$

Suppose j = 4:
$$
y_i = 
\begin{cases}
1 \text{ if } -\infty \leq y_i^* < \tau_1 \\
2 \text{ if } \tau_1 \leq y_i^* < \tau_2 \\
3 \text{ if } \tau_2 \leq y_i^* < \tau_3 \\
4 \text{ if } \tau_3 \leq y_i^* < \infty \\
\end{cases}
$$

### choice of a probabilty distribution function 

- as with binary dependent variables, we can choose the cumulative normal or cumulative standard logistic function
- the key difference is the assumption about the distribution of the error term ($\epsilon_i$)
- the results are similar with either approach
- in addition the coefficients, the cutpoints must also be estimated
- identificatoin comes from setting the intercept $\alpha = 0$ or setting $\tau_1 = 0$. Otherwise there are infinite equivalent solutions.

### Ordered Probit

Ordered probit uses the cumualtive normal. The probabilities are given as such:

$$
Pr(y_i = 1) = \Phi(\tau_1 - x_i \beta) \\
Pr(y_i = 2) = \Phi(\tau_2 - x_i \beta) - \Phi(\tau_1 - x_i \beta) \\
Pr(y_i = 3) = \Phi(\tau_3 - x_i \beta) - \Phi(\tau_2 - x_i \beta) \\
Pr(y_i = 4) = 1 - \Phi(\tau_3 - x_i \beta) \\
$$

For a value of $x_i \beta$, we get a probability estimate for each category.
Note: by subtracting $x_i \beta$ from a threshold, we convert it to a z-score. We can then find the area above/below it.

### Log Likelihood Function

$$
\begin{aligned}
L(\beta, \tau | y, X) = \prod_{i=1}^n p_i \\ 
= \prod_{j=1}^J \prod_{y_i=j} \prod(y_i = j | x_i, \beta, \tau) \\
= \prod_{j=1}^J \prod_{y_i=j} \Phi(\tau_j - x_i \beta) - \Phi(\tau_{j-1} - x_i \beta) \\
\end{aligned}
$$

Which makes the log-likelihood function:
$$
ln L(\beta, \tau | y, X) = \sum_{j=1}^J \sum_{y_i=j} ln [\Phi(\tau_j - x_i \beta) - \Phi(\tau_{j-1} - x_i \beta)]
$$

### Using software

- sata: `oprobit y x1 x2 x3`
- R: `model <- polr(y ~ x1 + x2 + x3, data = data, method = "probit", Hess = TRUE)`
    - using MASS library

### Basic Interpretation of Results
- there are three categories of the dependent variable, so there are two cut points estimated
- greater wealth and income inequality are associated with being higher up on the Freedom House Sscale (i.e. more free)
- greater oil production is associated with being lower on the scale
- being landlocked does not have a clear effect

### Some General Points
- To obtain predicted probabilities for each category, we need to specify values for the independent variables
- The same is true when we Want to explain how changes in X's affect these probabilities.
- It is thus useful to create relevant hypothetical cases that help illustrate the results.
- For example, choose X's that are substantively important, theoretically interesting, or very typical in the data.

### Hypothetical Case

Suppose we have a country with the following characteristics:
- GDP per capita: 10,000
- .5 barrels of oil produced per person/day
- not landlocked (0)
- Gini: 25

We can plug these values into the model to get the predicted probabilities for each category.
- The value of $x_i \beta$ is:
$$
.643 (9.21) - 1.23(0.5) - 0.65(0) + 0.030(25) = 6.11
$$
- The value of $\hat{\tau_1}$ is: 5.63 (this is from the estimates)
- The value of $\hat{\tau_2}$ is: 6.75
- Use the formulas above to get predicted probabilities

### Predicted Probabilities

$$
\begin{aligned}
Pr(y_i = 0) = \Phi(\hat{\tau_1} - x_i \beta) \\
= \Phi(5.63 - 6.11) \\
= 0.32 \\ 
Pr(y_i = 1) = \Phi(\hat{\tau_2} - x_i \beta) - \Phi(\hat{\tau_1} - x_i \beta) \\
= \Phi(6.75 - 6.11) - \Phi(5.63 - 6.11) \\
= 0.42 \\
Pr(y_i = 2) = 1 - \Phi(\hat{\tau_2} - x_i \beta) \\
\end{aligned}
$$

### Ordered Probit Post-Estimation

- In Stata, one can obtain the predicted probabilities for each case in the sample with the `predict` command:
    - `predict prob1, outcome(1)`
- or, create a variable with the predicted values of $x_i \beta$: `predict xbeta, xb`
- the `fitted.values()` function in R gives the predicted probabilities for each category

### Using the Margins command
- we can get estimates for:
    - predicted value of $x_i \beta$
    - predicted probabilities for each category of $y$
