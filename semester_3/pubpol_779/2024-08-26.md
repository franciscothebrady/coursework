## 2024-08-26

### Intro

Assumption is that you've had some experience with linear regression, and you're familiar with the concept of a linear model. This course will be about other methods that are used in other situations.

- panel structure data
- independent observations
- time series cross-sectional data

For this type of data, using linear regression leaves us vulnerable to errors.

Other techniques may be a little bit more niche, but its good to get an introduction to them. This course is for people who may be interested in doing higher level work, if you're doing program evaluation or research.

Resources to note:  
*  Scott Cunningham, Causal Inference: The Mixtape
*  Nick Huntington-Klein, The Effect

Problem Sets:
- 6 problem sets
- They are long!
Exams:
- Take home project
- Will take quite a long time!
- Decide on the model and interpret the results
- Some data manipulation
- Very different from a problem set!

Course:
- 2 halfs
- 1st half: Cross-sectional data
-- 2 parts: relevant advanced estimation methods and causal inference
- 2nd half: Time series data and time series cross-sectional data


### Maximum Likelihood Estimation

#### Some Notation

### Vectors:
- Array of numbers
Here is a row vector: $x \equiv [1 x1 x2 x3 . . . xk]$

Here is a column vector:
$\beta \equiv \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ ... \\ \beta_k \end{bmatrix}$

Thus, $x \beta = \beta_0 + \beta_1x_1 + \beta_2x_2 + . . . + \beta_kx_k$

### Matrices:
Essentially vectors with multiple rows or columns. A matrix is an array of numbers in rectangular format. For example, the matrix X has n rows and k columns. (Note: capital X is used to denote a matrix)

$X \equiv \begin{bmatrix} x_{11} & x_{12} & x_{13} & ... & x_{1k} \\ x_{21} & x_{22} & x_{23} & ... & x_{2k} \\ x_{31} & x_{32} & x_{33} & ... & x_{3k} \\ ... & ... & ... & ... & ... \\ x_{n1} & x_{n2} & x_{n3} & ... & x_{nk} \end{bmatrix}$

Note that indices for the elements of X are first row number and then column number. A vector is just a matrix that has either only one row or one column.

This is important because we can now represent our entire dataset as capital X.

### Some key matrices:

$y \equiv \begin{bmatrix} y_1 \\ y_2 \\ ... \\ y_n \end{bmatrix}$
- All of the outcomes of our dependent variable

$X \equiv \begin{bmatrix} x_{11} & x_{12} & ... & x_{1k} \\ x_{21} & x_{22} & ... & x_{2k} \\ ... & ... & ... & ... \\ x_{n1} & x_{n2} & ... & x_{nk} \end{bmatrix}$
- All of the Xs for the first row are the Xs for the first observation

$\beta \equiv \begin{bmatrix} \beta_0 \\ \beta_1 \\ ... \\ \beta_{k-1} \end{bmatrix}$
- This is our coefficient vector

$\epsilon_1 \equiv \begin{bmatrix} \epsilon_2 \\ \epsilon_3 \\ ... \\ \epsilon_n \end{bmatrix}$
- This is our error term

### Matrix Notation in this Class

In matrix form, we can express the dependent variable y as a linear function of our data:
$y = X \beta + \epsilon$

The above formula represents a whole stack of equations, one for each of the n items in a sample.

Boldface, lowercase letters indicate a vector. Boldface, uppercase letters are for matrices. Non-bolded letters represent a single number (scalar).

### Modeling the Data Generation Process (DGP)

What is the nature of the dependent variable y and the process for generating it?

- Discrete or continuous?
  - If discrete: binary or multinomial? Ordinal or nominal?
  - If continuous: what is the nature of its distribution?
    - Normally distributed?

- What is the structure of the data? Cross-sectional? Time-series? Panel?

### Preliminary

Note: $x \beta$ is a shorthand:
$x \beta = \beta_0 + \beta_1x_1 + \beta_2x_2 + . . . + \beta_kx_k$
- Usually, we’ve assumed that y is a linear function of x.
  - $y = x \beta + \epsilon$

- we have also **assumed** that $\epsilon$ has a normal distribution.
  - $\epsilon \sim N(0, \sigma^2)$

Together, these assumptions imply that y has a normal distribution with mean $x \beta$ and variance $\sigma^2$:
(In this situation!!!)

$y \sim N(x \beta, \sigma^2)$

### Preliminary, 2

- We’ve also employed a particular estimation method: Ordinary Least Squares
- OLS has an **analytic solution**: we take the derivative and find the values for $\beta$ that minimize the sum of the squared errors.
- Taking a step back, we should realize that:
  - y may *not* be a linear function of x
  - y may *not* be normally distributed
  - We can use other estimation methods that may not have an analytic solution

### Where do We Start?

- Suppose our dependent variable y consists of the following data points: 4, 7, 3, 5, 6, 5.
- We can theorize about what kind of DGP produced these data.
- There are many different probability distribution functions that are defined by different parameters. We choose one.
- MLE determines the values of the parameters that are most likely to have produced the observed data.
-- A choice of a probability distribution function, and estimating the parameters given the data we've observed.

### Example: Normal Probability Density Function

$f(y; \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(y-\mu)^2}{2\sigma^2}}$

- Suppose we believe our observations of y are generated by a normal distribution.
- Normal distributions are defined by the parameters $\mu$ and $\sigma$.
- The function gives the probability density for values of y as distributed around its mean.
- What are the values of the parameters most likely to have produced the values of y we observed?

### Maximum Likelihood Estimation (MLE)

- MLE is about identifying the values of the parameters, in this case $\mu$ and $\sigma$, that are most likely given the observed data.
- This simple scenario is not very interesting. The sample statistics $\bar{y}$ and $s$ are the best guesses for these parameters.
- It becomes more interesting when we represent the parameter $\mu$ as a linear function: $x \beta$
$f(y; \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(y-x \beta)^2}{2\sigma^2}}$
- MLE finds the values of $\beta$ that are most likely to produce the observed y’s given the x’s associated with these cases.

btw: you can do linear regression with MLE.

### A Binomial Distribution

- Suppose y is the number of bills reported out of committee from a total of G bills that were considered.
- Rather than come from a normal distribution, the probability distribution for y might be a binomial:

$Pr(y|\pi, G) = \frac{G!}{y!(G − y)!} \pi^y (1 − \pi)^{G-y}$

- The DGP includes an unknown parameter $\pi$ that we can estimate as a function of some independent variables.
- $\pi$ is the probability of success in a single trial.

### Forming a Likelihood Function

- In collecting data on Congressional bills, we will observe y and G. We would like to know $\pi$.
- We estimate this parameter by asking which value of $\pi$ is most likely given the data that we have observed.
- Suppose, in one Congress, G=15 and y=4:
$$
L(\pi|y = 4, G = 15) = \frac{15!}{4!(15 − 4)!} \pi^4 (1 − \pi)^{15-4}
$$

- The simple answer is that the likelihood is 4/15
- Let’s graph out the function as $\pi$ goes from 0 to 1.
- The likelihood is maximized when $\pi = .267$

### Methods for Finding the Maximum

- Analytic Approach: In this case, an analytic solution is possible by taking the derivative of the likelihood function with respect to $\pi$.
- Grid Search: Search over all possible sets of values of the parameters to find the maximum of the likelihood function.
- Search Algorithm: Use a “hill climbing” algorithm to identify at what parameter values the function reaches a maximum.

### What is Likelihood Function?

- A likelihood function looks like a joint probability distribution but it is not the same.
- A joint probability distribution gives the probability of getting a particular combination of outcomes given the parameter(s).
- What probability $\pi$ is the most likely given the data we've observed?

### Congressional Bill Example with More Data
- Suppose we now have data from multiple sessions of Congress.
- Each year, the probability of getting $y_i$ number of bills is:

$$
Pr(y_i|\pi, G_i) = \frac{G_i!}{y_i!(G_i − y_i)!} \pi^{y_i} (1 − \pi)^{G_i-y_i}
$$

- Therefore, the joint probability of observing a particular series of $y_i$’s is:

$$
Pr(y_1|\pi, G_1) \times Pr(y_2|\pi, G_2) \times Pr(y_3|\pi, G_3) \times . . .
= \prod_{i=1}^{n} Pr(y_i|\pi, G_i)
$$
- This still assumes that the probability of each bill passing is the same.

### It is more interesting if $\pi$ is re-parameterized

- Let $\pi = f(x \beta)$ that is bounded in the range 0 to 1:
$$
L(\beta |x, y, G) = \prod_{i=1}^{n} \frac{G_i!}{y_i!(G_i − y_i)!} f(x \beta)^{y_i} (1 − f(x \beta))^{G_i-y_i}
$$

The probability of reporting out a bill can now vary according to the characteristics of the congressional session measured in x.





<!-- Public Policy 779
Intro to Maximum Likelihood Estimation
Jonathan Hanson
Gerald R. Ford School of Public Policy
University of Michigan
August 26, 2024
Measurement and Sampling 1 / 30
Outline
1. Some Notation
2. Preliminary
3. Example: Normal
4. Example: Binomial
5. Likelihood Functions
Measurement and Sampling Outline 2 / 30
Outline
1. Some Notation
2. Preliminary
3. Example: Normal
4. Example: Binomial
5. Likelihood Functions
Measurement and Sampling Some Notation 3 / 30
Vectors
A vector is an array of numbers.
Here is a row vector: x ≡ [1 x1 x2 x3 . . . xk]
Here is a column vector:
β ≡
β0
β1
β2
...
βk
Thus, x β = β0 + β1x1 + β2x2 + . . . + βkxk
Measurement and Sampling Some Notation 4 / 30
Matrices
A matrix is an array of numbers in rectangular format. For example,
the matrix X has n rows and k columns.
X ≡
x11 x12 x13 · · · x1k
x21 x22 x23 · · · x2k
x31 x32 x33 · · · x3k
... ... ... . . .
xn1 xn2 xn3 · · · xnk
Note that the indices for the elements of X are first row number and
then column number. A vector is just a matrix that has either only
one row or one column.
Measurement and Sampling Some Notation 5 / 30
Some Key Matrices
y ≡
y1
y2
...
yn
X ≡
x11 x12 . . . x1k
x21 x22 . . . x2k
... ... . . . ...
xn1 xn2 . . . xnk
β ≡
β0
β1
...
βk−1
u1 ≡
u2
u3
...
sn
Measurement and Sampling Some Notation 6 / 30
Matrix Notation in this Class
Thus, in matrix form, we can express the dependent variable y as a
linear function of our data:
y = X β + u1
The above formula represents a whole stack of equations, one for
each of the n items in a sample.
Boldface, lowercase letters indicate a vector. Boldface, uppercase
letters are for matrices. Non-bolded letters represent a single number
(scalar).
Measurement and Sampling Some Notation 7 / 30
Outline
1. Some Notation
2. Preliminary
3. Example: Normal
4. Example: Binomial
5. Likelihood Functions
Measurement and Sampling Preliminary 8 / 30
Modeling the Data Generation Process (DGP)
What is the nature of the dependent variable y and the process for
generating it?
Discrete or continuous?
→ If discrete: binary or multinomial? Ordinal or nominal?
→ If continuous: what is the nature of its distribution?
Normally distributed?
What is the structure of the data? Cross-sectional? Time-series?
Panel?
Measurement and Sampling Preliminary 9 / 30
Preliminary
Note: x β is a shorthand:
x β = β0 + β1x1 + β2x2 + . . . + βkxk
Usually, we’ve assumed that y is a linear function of x.
y = x β +u1
We’ve also assumed that u1 has a normal distribution.
u1 ∼ N (0, σ2)
Together, these assumptions imply that y has a normal
distribution with mean x β and variance σ2:
y ∼ N (x β, σ2)
Measurement and Sampling Preliminary 10 / 30
Preliminary
We’ve also employed a particular estimation method: Ordinary
Least Squares
OLS has an analytic solution: we take the derivative and find
the values for β that minimize the sum of the squared errors.
Taking a step back, we should realize that:
→ y may not be a linear function of x
→ y may not be normally distributed
→ We can use other estimation methods that may not have an
analytic solution
Measurement and Sampling Preliminary 11 / 30
Where do We Start?
Suppose our dependent variable y consists of the following data
points: 4, 7, 3, 5, 6, 5.
We can theorize about what kind of DGP produced these data.
There are many different probability distribution functions that
are defined by different parameters. We choose one.
MLE determines the values of the parameters that are most
likely to have produced the observed data.
Measurement and Sampling Preliminary 12 / 30
Outline
1. Some Notation
2. Preliminary
3. Example: Normal
4. Example: Binomial
5. Likelihood Functions
Measurement and Sampling Example: Normal 13 / 30
Example: Normal Probability Density Function
f (y; μ, σ) = 1
σ√2π e− (y−μ)2
2σ2
Suppose we believe our observations of y are generated by a
normal distribution.
Normal distributions are defined by the parameters μ and σ.
The function gives the probability density for values of y as
distributed around its mean.
What are the values of the parameters most likely to have
produced the values of y we observed?
Measurement and Sampling Example: Normal 14 / 30
A Normal with μ=0 and σ=1?-12 -8 -4 0 4 8 12
Measurement and Sampling Example: Normal 15 / 30
A Normal with μ=-2 and σ=4?-12 -8 -4 0 4 8 12
Measurement and Sampling Example: Normal 16 / 30
A Normal with μ=7 and σ=2?-12 -8 -4 0 4 8 12
Measurement and Sampling Example: Normal 17 / 30
A Normal with μ=5 and σ=1.41?-12 -8 -4 0 4 8 12
Measurement and Sampling Example: Normal 18 / 30
Maximum Likelihood Estimation (MLE)
MLE is about identifying the values of the parameters, in this
case μ and σ, that are most likely given the observed data.
This simple scenario is not very interesting. The sample
statistics ¯y and s are the best guesses for these parameters.
It becomes more interesting when we represent the parameter μ
as a linear function: x β
f (y; μ, σ) = 1
σ√2π e−(y−x β)2
2σ2
MLE finds the values of β that are most likely to produce the
observed y’s given the x’s associated with these cases.
Measurement and Sampling Example: Normal 19 / 30
Outline
1. Some Notation
2. Preliminary
3. Example: Normal
4. Example: Binomial
5. Likelihood Functions
Measurement and Sampling Example: Binomial 20 / 30
A Binomial Distribution
Suppose y is the number of bills reported out of committee from a
total of G bills that were considered.
Rather than come from a normal distribution, the probability
distribution for y might be a binomial:
Pr(y|π, G) = G!
y!(G − y)!πy(1 − π)(G−y)
The DGP includes an unknown parameter π that we can estimate as
a function of some independent variables.
Measurement and Sampling Example: Binomial 21 / 30
Forming a Likelihood Function
In collecting data on Congressional bills, we will observe y and
G. We would like to know π.
We estimate this parameter by asking which value of π is most
likely given the data that we have observed.
Suppose, in one Congress, G=15 and y=4:
L(π|y = 4, G = 15) = G!
y!(G − y)!πy(1 − π)(G−y)
= 15!
4!(15 − 4)!π4(1 − π)(15−4)
Let’s graph out the function as π goes from 0 to 1.
Measurement and Sampling Example: Binomial 22 / 30
The Likelihood is Maximized when π = .267Likelihood
0 .1 .2 .3 .4 .5 .6 .7 .8 .9 1
pi
Measurement and Sampling Example: Binomial 23 / 30
Methods for Finding the Maximum
Analytic. In this case, an analytic solution is possible by taking
the derivative of the likelihood function with respect to π.
Grid Search. Search over all possible sets of values of the
parameters to find the maximum of the likelihood function.
Search Algorithm. Use a “hill climbing” algorithm to identify at
what parameter values the function reaches a maximum.
Measurement and Sampling Example: Binomial 24 / 30
Outline
1. Some Notation
2. Preliminary
3. Example: Normal
4. Example: Binomial
5. Likelihood Functions
Measurement and Sampling Likelihood Functions 25 / 30
What is Likelihood Function?
A likelihood function looks like a joint probability distribution
but it is not the same.
A joint probability distribution gives the probability of getting a
particular combination of outcomes given the parameter(s).
With a likelihood function, we know the outcomes and try to
identify which parameters are most likely given these outcomes.
We posit a particular DGP and use the data to identify the
parameters of the probability density function for that DGP.
Measurement and Sampling Likelihood Functions 26 / 30
Congressional Bill Example with More Data
Suppose we now have data from multiple sessions of Congress.
Each year, the probability of getting yi number of bills is:
Pr(yi|π, Gi) = Gi!
yi!(Gi − yi)!πyi (1 − π)(Gi−yi)
Therefore, the joint probability of observing a particular series of
yi’s is:
Pr(y1|π, G1) × Pr(y2|π, G2) × Pr(y3|π, G3) × . . .
=
n∏
i=1
Pr(yi|π, Gi)
Measurement and Sampling Likelihood Functions 27 / 30
Likelihood Function for Example
The likelihood function has become a bit more complicated:
L(π|y, G) =
n∏
i=1
Gi!
yi!(Gi − yi)!πy
i (1 − π)(Gi−yi)
Likelihood functions are easier to manage if we take the natural log:
ln L(π|y, G) =
n∑
i=1
ln [Pr(yi|π, Gi)]
The product of many probabilities becomes a very small number quite
fast. Taking the log converts it into a sum (of negative numbers).
Measurement and Sampling Likelihood Functions 28 / 30
It’s More Interesting if π is Re-Parameterized
Let π = f (x β) that is bounded in the range 0 to 1:
L(β |x, y, G) =
n∏
i=1
Gi!
yi!(Gi − yi)!f (x β)yi (1 − f (x β))(Gi−yi)
The probability of reporting out a bill can now vary according to the
characteristics of the congressional session measured in x.
Measurement and Sampling Likelihood Functions 29 / 30
Maximizing the Log Likelihood
Again, with MLE we find the parameter values that achieve the
highest possible log likelihood.
The variances of the parameters are estimated from the Hesssian
matrix, a matrix of second derivatives.
→ If the slope of the function changes very rapidly around its
maximum, the variance of the parameters is low.
→ If the function is flat around its maximum, we have little
precision.
Measurement and Sampling Likelihood Functions 30 / 30 -->