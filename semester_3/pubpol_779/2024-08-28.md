## 2024-08-28

### Dichotomous Dependent Variables

Likelihood functions are a lot like joint probability.
What is the process that is producing these outcomes that we're observing?
Write a join probability function that specifies some formula for the probability of observing the data that we have.
Objective is to maximize the probability that we observe the data that we have.

### Outline

1. Preliminaries: Limited Dependent Variables

### When Y has limited values
- we often face cases where y is dichotomous or comes in nominal/ordinal categories
- in these cases OLS can be problematic:
    - OLS gives continuous predicted values that may be hard to interpret
    - OLS can give predicted values that lie outside the range of the dependent variable
    - The effect of any $x_i$ on the outcome may not be linear across it's range
    - Heteroskedasticity is pretty much automatic in these cases

### Maximum Likelihood Approaches

- for dichotomous data, we have probit/logit which use different probability distributions, but produce similar results
- for nominal data (i.e. unordered categories), we have multinomial logit/probit
- for ordinal data, we can use ordered logit/probit
- in general: interpretation is key. it is much trickier than OLS; the coefficients do not directly tell us the magnitude of the effect of $x_i$ on $y$

### Example: Bias in Policy Professionals
- participants see the same data but the scenario differs

control group: a non-ideological scenario involving a study about the effectiveness of skin cream
treatment group: a study about the effects for the bottom 40% of the income distribution when localities raise the minimum wage.

- are people in the treatment group more likely to interpret the data correctly?
- dependent variable is 1 if the person interprets the results correctly, 0 otherwise

result: people are much more likely to interpret the data correctly in the skin cream scenario than in the minimum wage scenario.

### Results Table

- coefficient are not directly interpretable
- log-likelihood: you can compare the sparse model to the model with controls --> closer to 0 is better
- psuedo R^2: how much of the variance in the dependent variable is explained by the model -- this is not super useful.
- P: the probability that the coefficient is different from 0. comes from the likelihood ratio test, which compares the intercept-only model to the model with the coefficient in question. if the p-value is less than 0.05, we can reject the null hypothesis that the coefficient is 0.

### Dichotomous Data

Actually we can use OLS!

$$
E[y_i|x_i] = [1 x Pr(y_i = 1|x_i)] + [0 x Pr(y_i = 0|x_i)] \\
= Pr(y_i = 1|x_i)
$$

We can rerwrite the expected value of $y_i$ given $x_i$ as:

$$
Pr(y_i = 1|x_i) = x_i \beta
$$

Where $x_i$ is the vector of independent variables and $\beta$ is the vector of coefficients.

### Interpretation

- The interpretation is similar to OLS. Each one-unit increase in $x_i$ leads to a $\beta$ sized change in the probability that $y_i = 1$.
- problem: it's unrealistic that $x_i$ has the same linear effect on $y_i$ across the entire range of $x_i$.

### generalization to multiple independent variables

$$
Pr(y_i = 1|x_i) = x_i \beta \\
Pr(y_i = 1|x_i) = x_{i1} \beta_1 + x_{i2} \beta_2 + ... + x_{ik} \beta_k
$$

### Example: 2020 Presidential Election Choice

From the 2020 American National Election Study, we have data on who people voted for in the 2020 presidential election. The dependent variable is 1 if the person voted for Biden and 0 if they voted for Trump.

- if you run this model and then create the predicted values, you will get predictions that are outside the range of 0 and 1. This is a problem with OLS.

### In defense of LPM

- Easiert to interpret coefficients
- there are scenarios where a probit/logit cannot be estimated but an LPM can:
    - e.g. when an independent dummy variable is perfectly correlated with the outcome
- standard errors in a logistc model can be too small

### LPM works very well in RCT setup

- in an RCT with a dichotomous outcome, the LPM provides a consistent estimator of the average treatment effect:
$$
y = \beta_0 + \beta_1 T + \beta_2 X
$$

- in the above, both y and T are dichotomous, and $\beta_1$ is the estimated treatment effect.
- linearity doesn't matter since T has only two values. Also, x is uncorrered with T due to random assignment.

### Binary Response Models: Probit and Logit

- These techniques use an S-shaped curve that is bounded by 0 and 1 to fit the data.

Probit: based on cumulative normal distribution
Logit: based on cumulative logistic distribution

- In both, we think of the observed, dichotomous y as being generated by a latent (unobserved) variable that is continuous (y*).

$$
y = 1 if y_i^* > \tau \\
y = 0 if y_i^* \leq \tau
$$

These are called link functions because they link the latent variable (y*) to the observed variable (y).

### Choosing the distribution function
$$
y_i^* = x_i \beta + \epsilon_i
$$

- there aren't strong reasons to choose probit over logit. They yield similar results.
- in both, we model y_i* as a linear function of the $x_i$'s
- with probit, we assume standard normal errors: $\epsilon_i \sim N(0,1)$
- with logit, we assume logistic errors: $\epsilon_i \sim \lambda(0, \pi^2/3)$
    - logit has slightly fatter tails due to the greater variance

### Probit: Cumulative Normal Density Function

$$ 
\phi(\epsilon) = \int_{-\infty}^{\epsilon} \frac{1}{\sqrt{2\pi}} exp(-\frac{t-\mu^2}{2\sigma^2}) dt
$$

Which, since we assume $\mu = 0$ and $\sigma = 1$, simplifies to:

$$
\phi(\epsilon) = \int_{-\infty}^{\epsilon} \frac{1}{\sqrt{2\pi}} exp(-\frac{t^2}{2}) dt
$$

Here we use the cumulative normal density to get the area under the normal distribution up to $\epsilon$.

### Connection to using a z-table

- you should be familiar with looking up the area under the normal distribution above/below some z-score
- the cumulative standard normal density function gives the proportion of area below a z-score
- this function evaluated at 0 gives 0.5
- this function evaluated at 1.96 gives 0.975

### the latent variable: y*

y* is the linear function of the observed x's:
$$
y_i^* = x_i \beta + \epsilon_i
$$
There is some threshold value of y* above which $y_i = 1$:
$$
y_i = 1 if y_i^* > \tau \\
y_i = 0 if y_i^* \leq \tau
$$

We think of $y_i^*$ as being determined by $x_i \beta$ plus a stochastic component. The realization of $y_i^*$ is thus probabilistic to some degree. 

- We are using the fact that $\epsilon_i$ is normally distributed to get the probability that $y_i = 1$ given $x_i$.

### Probit Analysis

we use the latent variable $y_i^*$ and the cumulative normal density to calculate $Pr(y_i = 1|x_i)$, setting $\tau = 0$:
$$

Pr(y_i = 1|x_i) = Pr(y_i^* > 0|x_i) \\
= Pr(x_i \beta + \epsilon_i > 0|x_i) \\
= Pr(\epsilon_i > -x_i \beta|x_i) \\
= Pr(\epsilon_i \leq x_i \beta|x_i) \\
= \phi(x_i \beta)
$$

The probability that $y_i = 1$ given by the cumulative normal distribution evaluated at the value of $x_i \beta$.

### Probit: Maximum Likelihood Estimation

Let $p_i$ be the probability of observing the value of $y_i$ we observed:
$$
p_i = Pr(y_i = 1|x_i) if y_i = 1 \\
p_i = 1 - Pr(y_i = 1|x_i) if y_i = 0
$$

The likelihood function is thus:
$$
L(\beta|y,X) = \prod_{i=1}^{n} p_i
$$
We multiply together the respective probabilities of each observed value of $y_i$.

### Log likelihood function 

$$
L(\beta|y,X) = \prod_{i=1}^{n} p_i \\
= \prod_{y=1} Pr(y_i = 1|x_i) \prod_{y=0}[1 - Pr(y_i = 1|x_i)] \\
= \prod_{y=1} \phi(x_i \beta) \prod_{y=0}[1 - \phi(x_i \beta)]
$$

Which makes the log likelihood function:
$$
ln(L(\beta|y,X)) = \sum_{y=1} ln(\phi(x_i \beta)) + \sum_{y=0} ln[1 - \phi(x_i \beta)]
$$

### Estimation of Probit Model

- The MLE finds that values of the parameters $\beta$ that maximize the given log likelihood function.
- We get a set of coefficients just like usual. The interpretation is different, though.

- Stata: `probit y x1 x2 x3`
- R: `model <- glm(y ~ x1 + x2 + x3, family = binomial(link = "probit"), data = mydata)`

#### reading results 
- coefficients can *only* indicate direction, not magnitude

Next time: going through applied examples.