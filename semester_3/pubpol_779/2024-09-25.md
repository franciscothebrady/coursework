## 2024-09-25

### Illustrating Selection Bias with Math

$$
E[Y_i | z_m = 1] = E[Y_i | z_i* > 0] \\
= E[Y_i | v_i > -w_i\gamma] \\
= E[x_i \beta + \epsilon_i | v_i > -w_i\gamma] \\
= x_i\beta + E[\epsilon_i | v_i > -w_i\gamma] \\
= x_i\beta + \rho \sigma_{\epsilon} \lambda(-w_i\gamma) \\
= x_i\beta + \beta_{\lambda} \lambda(-w_i\gamma)

$$

Where $\lambda(-w_i \gamma)$ is an inverse Mills' ratio:

$$
\lambda_i(-w_i \gamma) = \frac{\phi(-w_i \gamma)}{1 - \Phi(-w_i \gamma)} \\ 
= \frac{\phi(-w_i \gamma)}{\Phi(w_i \gamma)}
$$

tldr regression has to include an extra term to account for selection bias.

### What we've learned thus far

- Selection bias can result when unmeasured factors are correlated with whether an observation ends up in our sample and with $y_i$.  
--> e.g. factors that affect whether one gets a job as well as the amount of wages offered.
--> e.g. factors that affect admission into grad school as well as performance in grad school.

- Since the inverse mills' ratio is correlated with these unmeasured factors, including it will help obtain unbiased estimates.

### Estimating a selection model

- One approach developed by Heckman, is a two-step process:
1. run a rpboti model for the selection equation. Use $\hat{\gamma}$ to estimate the inverse mills' ratio for each observation.
2. include the predicted inverse mills' ratio in the regression as an additional explanatory variable.

- Second, more efficient approach performs something similar in one step with MLE. This is the preferred approach.

### Building a likelihood function

- Has two components:
    - a normal distribution for the linear model when $z_i = 1$
    - a cumulative normal for the selection equation when $z_i = 0$

- the parameters in both parts are identified simultaneously.

### the log likelihood function

$$
ln L = \sum_{z=0} ln \Phi(-w_i \gamma) \\
+ \sum_{z=1} ln \phi(\frac{w_j \gamma + (y_j - x_j \beta) \rho/\sigma}{\sqrt{1 - \rho^2}})
- \sum_{z=1} \frac{1}{2} \frac{(y_j - x_j \beta)}{\sigma}^2
- \sum_{z=1} ln (\sqrt{2\pi \sigma })

$$

### Using software to estimate the model
Stata
```stata
heckman y x1 x2, select(z = x3 x4) 
```
Add the option `twostep` to use the two-step approach: probit followed by OLS.

R
```R
library(sampleSelection)
model <- heckit(z ~ w1 + w2, y ~ x1 + x2, data = data)
summary(model)
```

### Modeling the selection process

We need to identify at least one variable that helps predict selection into the sample but does not affect Y
- this requirement is known as the exclusion restriction.
- additionally, we assume that $\epsilon$ and $v$ have a bivariate normal distribution with a correlation of $\rho$.

### Limiations of the Heckman model
- the assumption that the errors have a bivariate normal distribution is strong.
    - if the assumption doesn't hold, the estimator is inconsistent.

- the results are often sensitive to specification changes.
- it can be difficult to find good predictors that meet the exclusion restriction.

### Endogenous Treatment Assignment

- we now turn to scenarios in which there is selection bias due to endogenous treatment assignment.
- there is not a random assignment of cases into the treatment and control samples.
    - e.g. we may think of treatment cases as being "self-selected"
- observable and unobservable factors that affect the outcome likely differ, on average, across the treated and untreated cases.
- with OLS, we can only control for observable factors.

### Basic set up 
- as with the sample selection models, there is a two-stage process: selection stage and outcome stage.

- the key difference is that we observe the value of the outcome for all cases, both treated and untreated.

- the selection stage is a probit model for treatment assignment.

- the key independent variable in the outcome stage is a dichotomous treatment indicator.

- again, we suspect that there are unmeasured factors that affect both treatment assignment and the outcome.

### models
- the outcome model is linear with $t_j$ being a dichotomous treatment indicator. the estimated effect of the treatment is $\delta$.

$$
y_j = x_i \beta + \delta t_j + \epsilon_j
$$

The selection process is the familiar probit setup:
$$
t_j \cases{1 & if w_j \gamma + u_j  > 0$ \\
0 & otherwise
}
$$

- unobservable factors that affect both selection and the outcome cause $\epsilon_j$ and $u_j$ to be correlated.
- by assumption, $\epsilon_j$ and $u_j$ have a bivariate normal distribution with a correlation at the level of $\rho$.

### Basic intuition behind approach 

- we use information from the first stage (probit) to estimate a "hazard" for each case.
- we can think of this hazard ($h_j$), roughly as a propensity for treatment.
- $h_j$ is included in the outcome (OLS) to control for the unobservable factors that bias the estimated treatment effect:  

$$
E[y_j|t_j,x_j,w_j] = x_j \beta + \delta t_j + \rho \sigma h_j
$$

- we seek to restore the conditional independence of $t_j$ and $y_j$.
- when estimated, the coefficient on $h_j$ represents $\rho \sigma$  

### full MLE implementation

- the original two-stage approach is fully implemented in a single MLE model, so $h_j$ does not appear as a parameter. 
- the likelihood function is more complicated.
- as with the heckman model, $\rho$ estimates the correlation between the disturbances.

(see slides for example and interpretation)